{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Numba\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook requirements\n",
    "\n",
    "This notebook assumes you have a Python environment with:\n",
    "\n",
    "* Python 3.9 or later\n",
    "* Numba 0.59 or later\n",
    "* SciPy\n",
    "\n",
    "The notebook should work on Windows, Mac, and Linux, but relative performance will depend on the specific hardware you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import __version__ as NUMBA_VERSION\n",
    "from scipy import __version__ as SCIPY_VERSION\n",
    "\n",
    "print('Numba:', NUMBA_VERSION)\n",
    "print('SciPy:', SCIPY_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does Numba exist?\n",
    "The idea surrounding Numba is to create the ability to extend Python with Python itself. It gets your code performance/capabilities that you just cannot otherwise achieve without C extensions/Cython/Pythran/&lt;insert extension creation package that calls a compiler here&gt;!\n",
    "\n",
    "## Conventions used throughout this tutorial series:\n",
    "\n",
    " * Tasks (exercises for the reader) are marked up in bold blue, like this:<h3><span style=\"color:blue\"> Task 1 </span></h3><br>\n",
    "\n",
    " * Hints/tips/suggestions/things to think about are in bold green, like this:\n",
    "<h4><span style=\"color:green\"> HINT: this is a hint </span></h4>\n",
    "\n",
    "\n",
    "## What is Numba?!\n",
    "\n",
    "**Numba is a type specialising just-in-time (JIT) compiler for Python functions.**\n",
    "\n",
    "It is most often used via a decorator...\n",
    "\n",
    "Simplest example with the simplest decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def add_one(x):\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above the ``jit`` decorator was imported and applied to a simple function (\"Numba is ... for Python functions\"). No compilation has occurred yet! Let's call the function with an integer:\n",
    "\n",
    "<h3><span style=\"color:blue\"> Task 1: call the `add_one` function with an integer input</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do this! Call `add_one` with an integer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ‰ Congratulations ðŸŽ‰, you just used one of the world's most powerful compliation chains (LLVM!)!\n",
    "\n",
    "Invoking the function makes compilation take place, this is what the just-in-time part is about (\"Numba is a ... just-in-time compiler\"). No compilation occurs until it is needed, and once it has occurred it is cached in process to be reused. Calling the function again with another integer will use the cached code path, it does not need to recompile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one(-129)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call function with a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one(12.34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This of course works, but a new specialisation was needed, Numba recompiled the function for floats.\n",
    "\n",
    "<h4><span style=\"color:green\">Remember: Numba is a compiler, specialisation leads to optimisation</span></h4>\n",
    "\n",
    "\n",
    "Each time a new **TYPE** of input is encountered a new specialisation for the function is being compiled. We can look at these via the `.signatures` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is what the type specilisation part means (Numba is a type specialising ... compiler).\n",
    "\n",
    "As the `.signatures` attribute is simply a list of signatures (each new signature is just appended to the list) it's easy to retrieve a particular function signature. For example, this selects the signature for the first specialisation (the one with the integer argument), it's at index zero as this type was encountered first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one.signatures[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It in fact has to work out the type of every variable based on the input types and the operations in the function, this is called \"typing\" a function and happens through a process called \"type inference\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Numba work?\n",
    "\n",
    "![How Numba Works](How_Numba_Works.png)\n",
    "\n",
    "#### <span style=\"color:green\"> TIP: Understanding type inference is really important, most errors you are going to see are because Numba couldn't figure out what type something should be. <br><br>Numba takes a function in a dynamic language and makes a type-static compiled verion of it!</span> \n",
    "\n",
    "## What did Numba do?!\n",
    "\n",
    "We've seen that Numba does multiple stages of transforms to go from bytecode to machine code. Three places that are important to look at are:\n",
    "1. The Numba intermediate representation (IR) level, this shows type information.\n",
    "   Access it via `.inspect_types` on a Numba JIT decorated function.\n",
    "2. The LLVM IR level, this shows the LLVM IR that Numba generated to pass to LLVM.\n",
    "   Access it via `.inspect_llvm` on a Numba JIT decorated function.\n",
    "3. The assembly level, this shows the disassembly of the compiled object.\n",
    "   Access it via `.inspect_asm` on a Numba JIT decorated function.\n",
    "\n",
    "Each of these methods takes a signature `kwarg` (like one obtained from `.signatures` above) as these representations are also type-specialised.\n",
    "\n",
    "<h3><span style=\"color:blue\"> Task 2: Using the `add_one` function from above, for the `float64` signature take a look at the various levels of transforms.</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first select the `float64` signature using the `.signatures` list attribute on the add_one function\n",
    "my_sig = # You write this!\n",
    "print(my_sig)\n",
    "\n",
    "# Level 1, print the types using `.inspect_types()`, make sure to specify the `signature` kwarg\n",
    "print(add_one.inspect_types())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 2, print the LLVM IR using `.inspect_llvm()`\n",
    "print(# You write this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 3, print the disassembly using `.inspect_asm()`\n",
    "print(# You write this!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "As we saw above, Numba JIT compiles specialisation of a function based on the input types. This means the first invocation is going to involve compilation, and subsequent are going to be from cache. Let's define a more involved function and look at how to benchmark Numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    # Implementation from https://en.wikipedia.org/wiki/Hypot\n",
    "    x = abs(x);\n",
    "    y = abs(y);\n",
    "    t = min(x, y);\n",
    "    x = max(x, y);\n",
    "    t = t / x;\n",
    "    return x * math.sqrt(1+t*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%timeit` magic is appropriate for timing Numba, it runs repeated loops so the cost of compiling the function is amortized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Numba `@njit` decorated functions have a `.py_func` attribute which holds reference to the pure Python function, we can use this to check the Python interpreter performance for the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hypot.py_func(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> **TIP: The most common mistake make when reporting performance results is reporting the execution time as the compilation time along with the execution time.** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops?!\n",
    "\n",
    "NumPy is amazing, but a side effect of the \"vectorisation\" paradigm is having to avoid loops. Numba is a compiler, compilers like loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 1000\n",
    "np.random.seed(0)\n",
    "X = np.random.rand((N * N)).reshape((N,  N))\n",
    "\n",
    "@jit\n",
    "def awkward_sine(a):\n",
    "    return np.imag(np.exp(1j * a))\n",
    "\n",
    "# check result\n",
    "np.testing.assert_allclose(awkward_sine(X), np.sin(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of the NumPy implementation running in the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit awkward_sine.py_func(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of Numba JIT version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit awkward_sine(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:blue\"> Task 3a: Reimplement the above `awkward_sine` function with a for loop using the template below...</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def awkward_sine_loops(a):\n",
    "    m, n = a.shape\n",
    "    result = np.empty_like(a)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            # You write this part!\n",
    "    return result\n",
    "\n",
    "# check result\n",
    "np.testing.assert_allclose(awkward_sine_loops(X), np.sin(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:blue\"> Task 3b: Check the performance of the loop based function above: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit awkward_sine_loops(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Performance\n",
    "\n",
    "Above we saw Numba JIT out perform NumPy in both a vectorised and a loop induced version. The loop version was quickest by some margin.\n",
    "\n",
    "#### <span style=\"color:green\"> Discussion: What might happen in this similar case? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def angle_to_complex(a):\n",
    "    return np.cos(a) + np.sin(a)*1j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of the NumPy implementation running in the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit angle_to_complex.py_func(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of Numba JIT version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit angle_to_complex(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we saw that loops made a difference, try that trick again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def angle_to_complex_loops(a):\n",
    "    m, n = a.shape\n",
    "    result = np.empty_like(a, dtype=np.complex128)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i, j] = np.cos(a[i, j]) + np.sin(a[i, j]) * 1j\n",
    "    return result\n",
    "            \n",
    "# time it\n",
    "%timeit angle_to_complex_loops(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips:\n",
    "\n",
    "Do:\n",
    " * Write good tests for the correctness of your code before using the JIT\n",
    " * Write a good performance harness for your code:\n",
    "   * Use real data/representative input\n",
    "   * Keep track of measurements\n",
    "\n",
    "Do not:\n",
    "\n",
    "* Include compilation time in the execution time.\n",
    "* Drag race tiny functions as an assessment of Numba's ability, Numba has dispatch cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_one(1)\n",
    "%timeit add_one.py_func(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drag race fundamental math operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def jit_cos(x):\n",
    "    return math.cos(x)\n",
    "\n",
    "%timeit jit_cos(12.34)\n",
    "%timeit jit_cos.py_func(12.34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Expect Numba to outperform the optimized linear algebra library (BLAS) used by NumPy and SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def dgemv(A, x):\n",
    "    m, n = A.shape\n",
    "    acc = np.zeros((m,))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            acc[i] += A[i, j] * x[j]\n",
    "    \n",
    "@jit\n",
    "def call_dot(A, x):\n",
    "    return np.dot(A, x) # Numba will defer this to BLAS::XGEMV\n",
    "\n",
    "%timeit dgemv(X, X[0])\n",
    "%timeit call_dot(X, X[0])\n",
    "%timeit call_dot.py_func(X, X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You wouldn't expect Fortran/C to do any better in the above situations, Numba is the same, internally it tries to make sensible decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The power of inlining functions for the compiler to use.\n",
    "\n",
    "Find the root of `f(x) = cos(x) + 1`. Use the Newton-Raphson algorithm. Use SciPy `optimize.newton` as a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def f(x):\n",
    "    return np.cos(x) + 1\n",
    "\n",
    "def dfdx(x):\n",
    "    return -np.sin(x)\n",
    "\n",
    "tol = 1e-7 # convergence tolerance\n",
    "max_it = 50 # maximum iterations\n",
    "x0 = 0.5 # starting guess\n",
    "\n",
    "%timeit optimize.newton(f, x0, fprime=dfdx, tol=tol, maxiter=max_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how Numba can further optimize this.  First we will write our own implementation of Newton-Raphson that Numba can compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def NR_root(f, dfdx, x0, max_it=max_it, eps=tol):\n",
    "    converged = False\n",
    "    for _ in range(max_it):\n",
    "        y = f(x0)\n",
    "        yprime = dfdx(x0)\n",
    "\n",
    "        if abs(yprime) < 1e-9:       # Give up if the denominator is too small\n",
    "            break\n",
    "\n",
    "        x1 = x0 - y / yprime            # Do Newton's computation\n",
    "\n",
    "        if abs(x1 - x0) <= eps:   # Stop when the result is within the desired tolerance\n",
    "            converged = True      # x1 is a solution within tolerance and maximum number of iterations\n",
    "\n",
    "        x0 = x1                         # Update x0 to start the process again\n",
    "    \n",
    "    if converged:\n",
    "        return x1\n",
    "    else:\n",
    "        raise RuntimeError(\"Solution did not converge\")\n",
    "\n",
    "# if the above is \"correct\", this will pass\n",
    "np.testing.assert_allclose(NR_root.py_func(f, dfdx, x0), np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit NR_root.py_func(f, dfdx, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the Python function with JIT compiled objective functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jit = jit(f)  # We can use jit like a regular function, without the @ decorator symbol.\n",
    "dfdx_jit = jit(dfdx)\n",
    "\n",
    "%timeit NR_root.py_func(f_jit, dfdx_jit, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the JIT compiled function with JIT compiled objective functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit NR_root(f_jit, dfdx_jit, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time SciPy with JIT compiled objective functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit optimize.newton(f_jit, x0, fprime=dfdx_jit, tol=tol, maxiter=max_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's really lean on the fact we have a specialising compiler... performance comes from specialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialize(f, dfdx):\n",
    "    # Close over the objective functions so they are \"constant\"\n",
    "    # Numba will spot this and LLVM will inline them into the machine code\n",
    "    @njit\n",
    "    def NR_root(x0, max_it=max_it, eps=tol):\n",
    "        # Copy the contents of NR_root above into this space.  Don't forget to indent it properly!\n",
    "    return NR_root\n",
    "\n",
    "f_NR_root = specialize(f_jit, dfdx_jit)\n",
    "\n",
    "%timeit f_NR_root(x0, max_it, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> Discuss: What happened in the above?!</span>\n",
    "#### <span style=\"color:green\"> TIP: Experiment and time things!</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
